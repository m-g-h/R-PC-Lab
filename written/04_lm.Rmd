---
title: "Linear Modelling"
output:
  ioslides_presentation:
    widescreen: yes
    slide_level: 2
  html_document:
    number_sections: yes
    toc: yes
    toc_float: no
---

```{r echo = F}
knitr::opts_chunk$set(echo = F)
```

# Introduction

## Workflow

```{r echo=FALSE}
DiagrammeR::grViz("figure/lm-modelling.gv", width=1000, height = 400)
```

## The `lm` Function

```{r eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
lm(formula,
   data,
   subset,
   na.action)
```

`formula` - Specification of our regression model

`data` - The dataset containing the variables of the regression

`subset` - An option to subset the data 

`na.action` - Option that specifies how to deal with missing values

The `formula` Argument
---

- Behind the scenes, OLS-estimation is linear algebra. E.g. The linear regression parameters are derived by $\boldsymbol{\beta} = \boldsymbol{(X'X)^{-1}X'y}$ 
- In order to keep our interaction with R short and simple (and without linear algebra), R offers the *formula method*

We can write our models using the following syntax:

```{r echo=TRUE, eval=FALSE}
model = formula(regressand ~ regressors)
```

Where `regressand` is just our dependent variable / response  usually denoted by $y$ and `model` is our formula of independent variables / regressors, e.g.:

```{r echo=TRUE, eval=FALSE}
happy_model = formula(happiness ~ age + income + n_children + married)
```

---

We can construct formulas with the following syntax:


- Adding variables with `+`
```{r eval=FALSE, echo = TRUE}
formula(y ~ a + b)
```

- Interactions with `:`
```{r eval=FALSE, echo = TRUE}
formula(y ~ a + b + a:b)
```

- Crossing: `a * b ` is equivalent to `a + b + a:b`
```{r eval=FALSE, echo = TRUE}
formula(y ~ a + b + a:b) # and
formula(y ~ a*b) # are equivalent
```

---

- Transformations with `I()`
```{r eval=FALSE, echo = TRUE}
formula(y ~ a + I(a^2)) # quadratic term must be in I() to evaluate correctly
formula(y ~ log(a)) # log can stay by itself

```
    
- Include all variables in your data with `.`
```{r eval=FALSE, echo = TRUE}
formula(y ~ .) # is equivalent to
formula(y ~ a + b + ... + z) # for a dataset with variables from a to z
```

- Exclude variables with `-`
```{r eval=FALSE, echo = TRUE}
formula(y ~ .-a ) # is equivalent to
formula(y ~ b + c + ... + z)# for a dataset with variables from a to z
```


The `subset` Argument
---

- Sometimes, we want to run our model on a subset of our data
- We can specify subsets of certain variables as follows:

```{r echo=TRUE, eval=FALSE}
lm(formula,
   data,
   subset = age < 30)
```

- Connect muliple subset arguments with logical operators:

```{r echo=TRUE, eval=FALSE}
lm(formula,
   data,
   subset = age < 30 & height > 180)
```

Note that although this works, a best-practice is to subset your data prior to the estimation. By keeping these steps distinct, your code will be much easier for someone else to understand.

The `na.action` Argument
---

If the data contains missing values, `lm` automatically deletes the whole observation.

- Specify `na.action = na.fail` if you want an error when the data contains missing values

Again, it is a best-practice to look for missing values in your data prior to the estimation to keep your code transparent.

- You can use the `missmap` function from the `Amelia` package to get a nice visualisation of missing values in your data


## Example Call of `lm` with Wage Data

```{r echo=TRUE}
wage_data <- read.csv2("assignment_wages/wages2.csv")

head(wage_data)

m1 <- formula(WAGE ~ EDUC + EXPER)

model<- lm(formula = m1,
           data = wage_data)
```

## Output of `lm`

The `lm` function returns a list. Relevant components of this list are:

- `call` - the function call that generated the output
- `coefficients` the OLS coefficients
- `residuals`
- `fitted.values` The estimates for our dpendent variable (WAGE)
- `model` The model matrix used for estimation

The full list of outputs can be looked up via

- `?lm()` 
- `str(model)` where model is our saved output from `lm`
- the `$` operator and `tab`, e.g. `model$...`

---

Lets look up our coefficients $\beta$, fitted values $\hat{y}$ and OLS residuals $\varepsilon$
```{r echo=TRUE}
model$coefficients
```

```{r echo=TRUE}
model$fitted.values[1:7] # first 7 fitted values
```

```{r echo=TRUE}
model$residuals[1:7] # first 7 residuals
```

---

We can visualise the results very simply with `hist` or `plot`:

```{r echo=TRUE, fig.width=10}
hist(model$residuals, breaks = 30)
```

---

```{r echo=TRUE, fig.width=10}
hist(model$fitted.values, breaks = 30)
```


## Output of `lm` with the `summary()` function

```{r}
summary(model)
```

## Display and Export Tables with `stargazer()`

```{r echo=TRUE, warning=FALSE}
stargazer::stargazer(model, type = "text", style = "asr" )
```

Export Stargazer Output to File
---
```{r, echo=TRUE, eval = FALSE}
stargazer::stargazer(model,
                     type = "html",
                     out = "model.html")
```

Compare different Models
---
```{r echo=TRUE}
model2 <- lm(WAGE ~ EDUC + EXPER + IQ + SCORES, data = wage_data)
stargazer::stargazer(model, model2, type = "text", style = "asr")
```


Specify the folder and file were your table should be saved as `"path/name.type"`

1. Output as `.html` : Open the file in your web browser and copy it into Word
2. Output as `.tex`  : Include in LaTeX

# Demonstration & Practice

# F-Test

## Motivation of the F-Test

We can test for significance of **single** parameters with t-tests

  - E.g. we can test if `EDUC` has an influence on `WAGE`
  - We can see if the effect of `EDUC` changes when more variables are added
    
We can test for joint significance of a group of variables with F-tests
 
   - E.g. are work-related variables like `TENURE`, `EXPER` and `SCORES` significant, once we control for personal     variables like `IQ` and `EDUC`
   
```{r echo=TRUE}
model3 <- lm(WAGE ~ IQ + EDUC, data = wage_data)
model4 <- lm(WAGE ~ IQ + EDUC + TENURE + EXPER + SCORES, data = wage_data)
```

## Model Comparison

```{r echo=TRUE, message=FALSE, warning=FALSE}
stargazer::stargazer(model3, model4, type = "text", style = "asr")
```

## Procedure of the F-Test
- Set up the models:
    - Restricted model: $WAGE = \beta_0 + \beta_{1}IQ + \beta_{2}EDUC$
    - Unrestricted model: 

$WAGE = \beta_0 + \beta_{1}IQ + \beta_{2}EDUC + \beta_{3}TENURE + \beta_{4}EXPER + \beta_{5}SCORES$

- State the Hypotheses that the *joint effect* of `TENURE`, `EXPER` and `SCORES` is zero:

$$ H_0: \beta_{3} = \beta_{4} = \beta_{5} = 0 \\
   H_1: H_0 \text{ is not true}$$

---

Since OLS minimises the $SSR$, the $SSR$ always increases if we drop variables. The question is, if that increase is significant.

- Calculate the test statistic:

$$ F = \frac{SSR_r - SSR_{ur}/q}{SSR_{ur} / (n-k-1)} \sim F_{q, n-k-1} \\
q = \text{number of restrictions} \\
k = \text{number of parameters}$$

If the $H_0$ is true, our test statistic follows the $F-Distribution$ and we can calculate p-values for our test.

## F-Test in R
```{r echo=TRUE}
anova(model3, model4)
```





