---
title: "Probability and Statistics Fundamentals (Session 2)"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
---

# Hello
<span style="color:red">Blabla this section introduces you to bla</span>

# Random Variables
Random variables are variables that take on numerical values and are determined by experiments. Mathematically speaking, probability experiments (or trials) are procedures which can be infinitely repeated and which have a well-defined set of possible outcomes. These outcomes are affected by chance.

Take for example a simple coin toss experiment. The outcomes are well defined by $\{heads, tails\}$ which we we can assign (recode) to $\{0,1\}$ meaning "no heads" and "heads". The variable for the outcome will be denoted by $X$. Now, unless we perform a cointoss, the value $X$ takes on remains essentially *random*.



## A few notational remarks
Consider the vector $X_i = (X_1 , X_2, \dots, X_{100})$ which contains the results from a fair coinflip. Using the `rbinom` function we simulate `n = 100` cointosses and obtain a vector of results $x_i = (x_1, x_2, \dots, x_{100})$ :

```{r}
X <- rbinom(n = 100, size = 1, prob = 0.5)

print(X)

```
 
If we would additionally conduct another experiment where we count two successive coin tosses (`size = 2`), we would obtain a secon random variable, which we would denote by $Y_i$ and $y_i$ respectively.

**Notation:**

- Use **capital letters** such as $O$ or $P$ to denote a **random variable**
- Use **small letters** such as $m$ or $n$ to denote **particual outcomes** of $M$ and $N$
- Use **subscripts** (e.g. $a_i$) to distinct **different outcomes** of the same random variable $A_i$
<span style="color:red">\\ben: $a_i$ kann auch benutzt werden, um outcomes verschiedener variablen $A_i$ zu denoten (s. wooldridge)</span>

## Discrete random variables
Discrete random variables are variables which can only take a countable number of values. For our cointoss example, $X$ is a random variable with $j = 2$ outcomes, $\{0, 1\}$ (no head, head). Then the probability of each outcome is:

$$p_j = P(X = x_j), \; j = 1, 2 \qquad \text{with } p_1 + p_2 = 1$$

Random variables which can only take on two values are called **Bernoulli random variables**

### Example:drawing a card and predicting its symbol 

- Denote the random variable for the card draw by $C$
- We have $j = 4$ possible outcomes for $C$, $\{hearts, spades, clubs, diamonds\}$
- The probabilities are denoted as: $p_j = P(C = c_j), \; j= 1, 2, \dots , 4$
- Since we know that each symbol is represented equally in the deck, $p_j = 0.25$ for all $j$.
- The probability to draw a joker is $0$ since the sample space only includes the four symbols.
- Likewise, the probability to draw any of the four symbol is $1$ ($100%$).

## Continuous random variables
We call a random variable continuous if it takes on any real value with a probability of $0%$. This is because the variable could essentially take on an infinity of values and $100%$ divided by infinity is (~~not possible~~) approaching $0%$. Continuous random variables are seldom observed in everyday life, but one pseudo-example is the height of a person. Usually a person's height ranges somewhere between $45cm$ and $272cm$. It is true that it is unlikely that someone's height would take on a negative value or one that is generally outside the above-mentioned range. However, within the range, there is still an infinity of exact values that a person's height can take on (consider my height: $182,6732467cm$, still taller than my neighbour, who measures $182,6732466cm$).


# Probability Density Functions and Cumulative Density Functions

## Discrete random variables
For discrete random variables, the probability density function (pdf), or rather, the probability mass function (pmf) summarises the information concerning the possible outcomes of a discrete random variable $X$ and its corresponding probabilities. It is defined as: $$f_X(x) = P(X=x)$$ For our card symbol example, a pmf would look like this:
```{r warning=FALSE, fig.height=4, fig.width=9}
library(ggplot2)

p <- rep(0.25,4)
symbols <- factor(c(0,1,2,3), labels=c("Hearts","Spades",
                                         "Clubs", "Diamonds"))
df = data.frame(p, symbols)
ggplot(df, aes(symbols,p, fill=symbols)) + 
  geom_bar(stat="identity",width = 0.05) +
  scale_y_continuous(limits=c(0,1)) + labs(y="f(x)", x="Symbols") +
  scale_fill_manual("legend",
                    values = c("Hearts" = "red", "Spades" = "black",
                               "Clubs" = "black", "Diamonds" = "red")) + 
  guides(fill=F) + theme_minimal()
```

With this pmf, we can now simply calculate the probability of any event involving that variable. For instance, we could ask ourselves: "What is the probability that the symbol of the card that I will draw is either Hearts or Clubs?". The pmf tells us: $$\begin{aligned}&P(X=\text{Hearts or } X=\text{Clubs})\\&=P(X=\text{Hearts}) + P(X=\text{Clubs})\\&=0.25 + 0.25 = 0.5\end{aligned}$$  
We could also answer that question with the help of the cumulative density function (cdf). As the name already suggests, the cdf cumulates the probabilities of the single possible outcomes and is defined as $$F(x) = \sum_{X\leq x}f_X(x)=P(X\leq x)$$ Our current example with the cards does not really suit the benefits of a cdf, as we could indeed read from the function the probability of drawing either a Hearts card or a Spades card, but we are unable to read from it the probability of drawing either a Diamonds card or a Hearts card:
```{r warning=FALSE, fig.height=4, fig.width=9}
df$xend <- c(df$symbols[2:nrow(df)], 5)
df$yend <- cumsum(df$p)
ggplot(df, aes(as.numeric(symbols), cumsum(p), xend=xend, yend=yend, color=symbols)) +
      #geom_vline(aes(xintercept=as.numeric(symbols)), linetype=2, color="grey") +
      geom_point() +
      geom_point(aes(x=xend, y=cumsum(p)), shape=1) + 
      geom_segment() +
      scale_color_manual("legend",
                    values = c("Hearts" = "red", "Spades" = "black",
                               "Clubs" = "black", "Diamonds" = "red")) +
      scale_x_continuous(breaks = 1:5, 
                         labels = c("Hearts", "Spades",
                                    "Clubs", "Diamonds", "")) +
      guides(color=F) + labs(x="Symbols", y="Probability") +
      theme_minimal()

```

## Continuous random variables

Because it makes little sense to think about the probability that a continuous random variable assumes a certain value, the pdf of continuous random variables is used to calculate events concerning a range of values. This is done by taking the integral of this range. Generally: $$F_X(x) = \int f_X(x) dx$$

In our height example, we could be interested in the probability that a person is somewhere between $170cm$ and $180cm$ tall, i.e. in $P(170\leq X \leq 180)$.
```{r warning=FALSE, fig.height=4, fig.width=9}
height = rnorm(10000, mean=177, sd=30)
df = data.frame(height)
dp <- ggplot(df, aes(height)) + geom_density(size=1) + 
  geom_vline(aes(xintercept=mean(height)), color="red", linetype="dashed",
                 size=1)
dpb <- ggplot_build(dp)
x1 <- min(which(dpb$data[[1]]$x >=170))
x2 <- max(which(dpb$data[[1]]$x <=180))
dp + geom_area(data=data.frame(x=dpb$data[[1]]$x[x1:x2],
                               y=dpb$data[[1]]$y[x1:x2]),
               aes(x=x, y=y), fill="blue", alpha=0.1) +
    theme_minimal() + 
    theme(axis.title.y=element_blank(),
          axis.text.y=element_blank()) + labs(x="Height")
```

```{r warning=FALSE, message=FALSE}
library(sfsmisc)
dens = density(df$height)
integrate.xy(dens$x, dens$y,170,180)
```
Thus, $P(170\leq X \leq 180) \approx 0.13 = 13%$ (the integrate.xy function is not exact).

While we can calculate probabilities under the pdf for a given Range $P(a \leq X \leq b)$, the cumulative density function always provides us with the probability of the variable to take on a value *up to* $x$, that is, $P(X \leq x)$. Its definition is: $$F_X(x)=\int_{-\infty}^xf_X(x)dx$$ At all points its value is the total area under the curve of the pdf up to $x$.

```{r warning=FALSE, fig.height=4, fig.width=9}
df_int = data.frame(height=dens$x, y=cumsum(dens$y)/2)
ggplot(df_int, aes(height,y)) + geom_line(size=1) + 
  geom_vline(aes(xintercept=mean(height)), color="red", linetype="dashed",
                 size=1) +
  scale_x_continuous(breaks=seq(50,300,50)) + 
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  theme_minimal() + labs(x="Height", y="F(x)")
```
From this graph we can now easily read the probability that a person is $150cm$ tall or smaller (ca. $20%$). For the requirements and attributes of probability density functions and cumulative density functions, please refer to the course slides.

# Joint Distributions and Conditional Distributions
We often have to deail with more than just one variable in applied econometrics. For example, we could be interest in the probability that a person is between $170cm$ and $180cm$ tall *and* weighs between $60kg$ and $90kg$ (this is important for shoe manufacturers). We could also be interested in the probability of getting a $6$ with one throw of a die given the die is red instead of blue (comes in handy at the casino).

## Joint Distributions

### The discrete case
Let $X$ be a random variable for the throw of a die with equal chances to every side, and $Y$ a random variable for the color (blue, red) of the die. Throwing a random die would lead us to the joint probability density function of $(X,Y)$: $$f_{X,Y}(x,y) = P(X=x, Y=y)$$.

It is quite easy to obtain the joint pdf of $X$ and $Y$ if they are independent. $X$ and $Y$ are independent if: $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$ In other words, they are independent if the die side thrown does not depend on the color of the die. In the discrete case, it is the same as $$P(X=x, Y=y) = P(X=x)P(Y=y)\text{,}$$ that is, the probability that the die is read and you throw a 6 is the product of the probability of throwing a 6 and of the die being red. Since we know these probabilities, we can calculate: $\frac{1}{6} \times \frac{1}{2} = \frac{1}{12}$.

### The continuous case
By taking the integral of a joint pdf of continous variables, we can get the probability that X and Y fall in a certain interval: $$P(a \leq X \leq b, c \leq Y \leq d) = \int_a^b \int_c^df_{X,Y}(x,y)dydx$$
If you have any given functions, it is quite easy (as with the normal distribution) and can be done by hand (might in some cases be easier than doing it with r). A continuous joint pdf could look like this ($X$ = height, $Y$ = weight; this is a bad example because obviously weight is dependent from height).
```{r warning=FALSE, message=FALSE, fig.height=4, fig.width=9}
library(MASS)
library(threejs)
# Lets assume different standard deviations for height
# so that we are only looking at grown men and women
mu = c(177, 70); names(mu) = c("X", "Y")
stdevs = c(4, 5); names(stdevs) = names(mu)
cormatrix = matrix(c(1,0.9,0.9,1), ncol=2)
b = stdevs %*% t(stdevs)
varcovmat = b*cormatrix
colnames(varcovmat) = rownames(varcovmat) = names(mu)
obs = mvrnorm(n=10000, mu=mu, Sigma=varcovmat, empirical=T)
obs.kde = kde2d(obs[,1], obs[,2], n=100)

x = obs.kde$x; y = obs.kde$y; z = obs.kde$z
xx = rep(x, times=length(y))
yy = rep(y, each=length(x))
zz = z; dim(zz) = NULL
ra = ceiling(16*zz/max(zz))
col = rainbow(16, 2/3)
scatterplot3js(x=xx, y=yy, z=zz, size=0.4, color=col[ra], bg="white")
```

## Conditional Distributions
When we are interested in the probability of a specific outcome given we observed another specific outcome, we might want to take a look at the conditional distribution of $Y$ given $X$, or rather, at the **conditional probability density function**: $$\frac{f_{Y|X}(y|x)}{f_X(x)}$$
For the discrete case follows: $$f_{X|Y}(y|x) = P(Y=y|X=x)$$
When $X$ and $Y$ are independent, it follows from $f_{X,Y}(x,y) = f_X(x)f_Y(y)$ that $$f_{X|Y}(y|x) = f_Y(y)$$

$f_{X,Y}(x,y) = f_X(x)f_Y(y)$


# Features of Probability Distributions

## Central Tendency: The Expected Value

The most commonly used feature of a probability distribution is the *expected value*. It splits the probability under the *pdf* in half.

> The expected value is the value from a probability distribution, for which the probabilities of another value being greater or smaller than it are exactly equal.

For example, we simulate a random variable $R$ from a standard normal distribution. First, we simulate the variable and draw a histogram with the arithmetic mean:
```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337) #seed for reproducibility
data <- data.frame(R = rnorm(n = 1000000))

ggplot(data, aes(x = R)) +
  geom_histogram(aes(y = ..density..), bins = 200) + 
  geom_vline(xintercept = mean(data$R), color="red") +
  theme_minimal()
```

It looks like the area to the left of the mean is the same size at the area on the right side. Let's check that:

```{r}
A <- numeric() # Variable for storing the area sizes

A[1] <- (sum (abs (data$R[data$R < mean(data$R)]))) /sum(abs(data$R)) # Area to the left of the mean in percent
A[2] <- (sum (abs (data$R[data$R > mean(data$R)]))) /sum(abs(data$R)) # Area to the right of the mean in percent

print(A)
```

As we can see (and calculate), the area sizes are very similar, meaning htat half of the probability mass is on each side of the mean. Notice that since we're in a simulation context, the areas are not *exactly* the same.

### Useful Rules for Expected Values

For any constants $a$ and $b$ and a random variable $X$, the following rules apply:

- The expected value of a constant is the constant
$$ \mathbb{E}[c] = c  $$
- The expected value of a linear function of a random variable is the linear function of the expected value of the variable 
$$ \mathbb{E}[aX+b] = a\mathbb{E}[X] + b$$

- The expected value of a sum of random variables is the sum of the expected values of these random variables:
$$ \mathbb{E}\left[ \sum_{i=1}^n a_iX_i \right] = \sum_{i=1}^n \mathbb{E}[a_iX_i] = \sum_{i=1}^n a_i \mathbb{E}[X_i] $$
- For a variable $Y$ that is independent of $X$ it holds that:
$$ \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[X] $$
- The population mean is often denoted by $\mu$ or $\mu_X$ instead of $\mathbb{E}[X]$

## Variablity: Variance and Standard Deviation

The central tendendcy tells us, around which value the outcomes of the random variable cluster, but it is also important to get a measure on how far they *spread around the mean*. The variance and standard deviation are measures for this.

You can see this notion of *distance from the mean* in the definition of the variance:

$$ Var[X] = \mathbb{E} \left[ \left( X - \mathbb{E} [X] \right) ^2  \right] $$

The deviation from the expected value $(X - \mathbb{E}[X])$ is squared in order to prevent perfect cancellation of deviations, since:
$$\mathbb{E} \left[(X - \mathbb{E}[X]) \right] = \mathbb{E}[X] - \mathbb{E}[X] = 0$$

### Useful Rules for Variances

For any constants $a$ and $b$ and a random variable $X$, the following rules apply:

- Alternative representation
$$ Var[X] = \mathbb{E} \left[ X^2 \right] - \mathbb{E}[X]^2 $$
- Variance of a constant is zero:
$$ Var[c] = 0 $$
- Variance of a linear combination:
$$ Var[aX+b] = a^2 Var[X] $$
- The population variance is often denoted by $\sigma^2$ or $\sigma_X^2$ instead of $Var[X]$.

### The Standard Deviation
If we change the unit of measurement of $X$ by $a = 1000$, e.g. from kilometers to meters, the variance increases linearly by $a^2 = 1000 * 1000$ (see rules above). Since this makes it difficult to compare the variability of different variables, we use the standard deviation, which is the positive part of the square root of the variance:

$$ sd[X] = + \sqrt{Var[X]} $$
For any constants $a$ and $b$ and a random variable $X$, it has the following useful properties:
- Standard deviation of a linear combination:
$$ sd[a X + b] = |a| sd[X] $$

- The standard deviation is often denoted by $\sigma$ or $\sigma_X$ instead of $sd[X]$.

## Standardisation of Variables

Using the properties of the expected value and variance, we can transform any variable to a standardised variable with mean 0 and standard deviation 1:
$$ Z := \frac{X-\mu}{\sigma} $$

We can demonstrate this with a short simulation. In the following we simulate a random variable $X$ from a normal distribution, that has mena $3$ and standard deviation $1.5$

```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337) #seed for reproducibility
data_st <- data.frame(R = rnorm(n = 10000, mean = 3, sd = 1.5))

ggplot(data_st, aes(x = R)) +
  geom_histogram(aes(y=..density..),bins = 100) + 
  geom_vline(xintercept = mean(data_st$R), color="red") +
  geom_vline(xintercept = 0, color="green") +
  stat_function(fun = dnorm, color = "green", args = list(mean = 0, sd = 1))+
  stat_function(fun = dnorm, color = "red", args = list(mean = 3, sd = 1.5)) +
  xlim(c(-4,8)) +
  theme_minimal()
```

The green lines indicate a standard normal distribution with $\mu = 0$ and $\sigma = 1$. Our simulated varaible clearly deviates from it. If we now transform $X$ according to the formula above, the result yields:

```{r warning=FALSE, fig.height=4, fig.width=9}
data_st$Z <- (data_st$R - mean(data_st$R)) / sd(data_st$R)

ggplot(data_st, aes(x = Z)) +
  geom_histogram(aes(y=..density..),bins = 100) + 
  geom_vline(xintercept = mean(data_st$R), color="red") +
  geom_vline(xintercept = 0, color="green") +
  stat_function(fun = dnorm, color = "green", args = list(mean = 0, sd = 1))+
  stat_function(fun = dnorm, color = "red", args = list(mean = 3, sd = 1.5)) +
  xlim(c(-4,8)) +
  theme_minimal()
```

## Association: Covariance and Correlation

The covariance captures the joint variation of two random variables. It allows us to get a sense of the relationship of them, e.g. $X$ is always high when $Y$ is. This can be visualised graphically:

```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337)
data_cov <- data.frame(X = 1:1000 + rnorm(1000, sd = 100),
                       Y = 1:1000 + rnorm(1000, sd = 100))

ggplot(data_cov, aes(x = X, y = Y)) + 
  geom_point() +
  geom_point(aes(x = mean(X), y = mean(Y)), color ="red") + 
  geom_vline(xintercept = mean(data_cov$X) + sd(data_cov$X), color = "red") +
  geom_hline(yintercept = mean(data_cov$Y) + sd(data_cov$Y), color = "red")
```

We can easily see that $X$ and $Y$ have a strong relationship. We can stat that: when the deviation of $X$ (vertical line) from its mean (red dot) is high, the same can be said about the deviation of $Y$ (horizontal line).

Expressing this relation of deviations from the mean mathematically yields us the formula of the covariance:

$$ Cov[X,Y] = \mathbb{E} \left[ (X - \mathbb{E}[X]) (Y - \mathbb{E}{Y} ) \right] $$

### Useful Rules for Covariances

For any constants $a$ and $b$ and random variables $X$, and $Y$ the following rules apply:

- Alternative representations:
$$ \begin{aligned}
Cov[X,Y] &= \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \\
&= \mathbb{E}[(X -\mathbb{E}[X])Y] \\
&= \mathbb{E}[X (Y -\mathbb{E}[Y])] \\
\end{aligned}$$

- Covariance given independence of $X$ and $Y$:
$$ \begin{aligned}
Cov[X,Y] &= \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \\
&= 0 & \text{since } \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[X] 
\end{aligned}$$

- Variance of linear combinations:
$$ Cov[a_1 X + b_1, a_2Y + b_2] =  a_1 a_2 Cov[X,Y]$$

- The boundary of the covariance of two random variables (its most extreme value) is given by the product of their standard deviations. This is also called the *Cauchy-Schwartz Inequality*:
 $$ |Cov[X,Y]| \leq sd[X] sd[Y] $$
 
 - The population covariance is usually denoted by $\sigma_{XY}$ instead of $Cov[X,Y]$
 
### The Correlation Coefficient

The variance and standard deviation are both dependent on their units of measurement. To capture the *pure* relationsip betwee two random variables, we can get rid of this by using the *Cauchy-Schwartz Inequality* to normalise the variance. This yields us the correlation coefficient:

$$  Corr[X,Y] = \frac{Cov[X,Y]}{sd[X] sd[Y]} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$

- The population correlation coefficient is usually denoted as $\rho$ or $\rho_{XY}$ instead of $Corr[X,Y]$

# Simple Linear Regression and OLS
Now that we have learned about correlation and testing, we want to predict one variable from another. One simple way to do so is using linear regression. Simple linear regression (SLR) is about predicting a dependent variable (*regressand*) from one independent variable (*regressor*). A SLR model is given by: $$y = \beta_0 + \beta_1x + \epsilon$$
where $y$ is the regressand and $x$ the regressor. $\epsilon$ denotes the error of the model, i.e. the residuals. Usually, researchers are interested in the amount of change in $y$ that occur when $x$ changes. As in any linear function, the rate of change is given by $\beta_1$. More formally: $$\frac{\delta y}{\delta x} = \beta_1$$

## Explanation by example
Now what does a linear regression model tell us? It tells us the relation between two variables and in the best case lets us predict the outcome if we have information about the independent variable. Assume that we have ten students who took exams in statistics and exams in econometrics. We know what grades were in percentage points. Now, we would like to predict the econometrics grade of any other student who took the statistics exam. Our model is as follows:

$$econgrade = \beta_0 + \beta_1statsgrade + \epsilon$$

```{r fig.height=4, fig.width=9}
library(ggplot2)
student_id = c(1,2,3,4,5)
x = c(95,85,80,73,70,65,60,58,55,51)
y = c(85,95,75,70,65,70,62,53,52,51)
grades_df = data.frame(student_id, x, y)
ggplot(grades_df, aes(x, y)) + 
  geom_point() + labs(x="Statistics Grade", y="Econometrics Grade") + geom_smooth(
    method="lm", colour="red", se=T, alpha=0.1) + theme_minimal()
```
```{r}
summary(lm(y~x, data=grades_df))$coefficients
```
Judging from the graph, we have a clear linear relationship between the students' statistics grades and the econometrics grades. In fact, our $\beta_1$, which is also the line's slope, can be read from the summary ($0.939$) and from the Pr-column we also see that this estimate is different from zero (statistically significant, ***). A standard interpretation of this information would go as follows: "Ceteris paribus (with all else being equal, an increase of $1$ of the statistics grade implies an increase of $0.939$ of the econometrics grade. Given the t value which is above our given threshold of $1.96$, we would reject our null hypothesis that the influence of the statistics grade is equal to $0$." We may go even as far and state that if a current student in the econometrics class has achieved a grade of $90$, he will approximately score $87$ points in the econometrics exam.  But we have an essential problem with this interpretation, because we do not actually know that we have a causal relationship here or just a correlation. And right now, we do not know whether the line we drew represents the best guess for a prediction. To answer these questions, we need to know more.


## Law of Iterated Expectations
We will shortly learn under which conditions we can assume that we're looking at a causal relationship of $x$ on $y$. But before we do that, we need to know and understand the Law of Iterated Expectations (LEI). It states that the Expectation of $y$ given $x$ is equal to the expectation of $y$:
$$\mathbb{E}\left[Y\right] = \mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]$$
To get an idea why this holds, consider a game of darts being played in another room and you want to guess the landing spot of a dart. Let $Y$ denote this spot. Without any prior information, our best guess for the value $Y$ takes on would be the center $c$ of the board: $$\mathbb{E}\left[Y\right] = c$$
Now suppose that every time a dart is thrown, someone watching in the other room tells us whether the dart landed in the upper or lower half of the board. Let this be another random variable $X$. By the information of $X$ we can improve our guess for the landing spot - if $X = upper$, then our best guess for $Y$ might be the center of the upper half of the board, $\mathbb{E}\left[Y|X = \text{upper}\right] = c^{\text{upper}}$, if $X = lower$, then our best guess for $Y$ would be the center of the lower half of the board, $\mathbb{E}\left[Y|X = \text{lower}\right] = c^{\text{lower}}$
Thus, the expectation of $Y$ conditional on $X$ depends on whether $X = up$ or $X = low$. But since $X$ is a random variable, $\mathbb{E}\left[Y|X\right]$ is also a random variable.
When iterating Expectations like $\mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]$ we take expectations over the random variable $\mathbb{E}\left[Y|X\right]$, i.e., averaging the two best guesses of $X$ (upper and lower). When averaging those guesses proportional to how likely each $X$ outcome will be, then we expect that the averaged guess $\mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]$ agrees with the overall "direct" best guess $\mathbb{E}\left[Y\right]$

Credits to Jimmy Jin for the great intuition behind the LEI.

## Causal interpretation
For a regression to be more than a simple correlation, one has to make assumptions how $x$ is related to the error term $\epsilon$. For causal interpretations, $x$ must not include information about $\epsilon$, that is, they must not be correlated. If they would be correlated, it is still possible that something else than $x$ predicts $y$. Mathematically speaking: $$\mathbb{E} \left[\epsilon|x\right] = 0$$
This is called the **zero conditional mean assumption**. If it holds, we have **exogeneity**. In the example with the statistics and econometrics grade, we obviously do not have exogeneity, as the statistics grade is for example also highly correlated with the number of hours spend studying, that may also affect the econometrics grade. In this case, $\mathbb{E}\left[\epsilon|statsgrade\right] \neq 0$

Following from this and the Law of Iterated Expectations: $$\begin{aligned}\mathbb{E} \left[\epsilon|x\right] &= \mathbb{E}\left[\epsilon\right] = 0\text{,}\end{aligned}$$ therefore: $$\begin{aligned}Cov\left[x, \epsilon\right] &= \mathbb{E}\left[x\epsilon\right] - \mathbb{E}\left[X\right] \mathbb{E}\left[\epsilon\right]  = 0\end{aligned}$$

Effectively, if the expected value of the error term is zero, the model is without error and thus average value of $y$ can be expressed as a linear function of $x$:
$$\begin{aligned}\mathbb{E}\left[y|x\right] &= \mathbb{E}\left[\beta_0 + \beta_1x + \epsilon|x\right] \\ &= \beta_0 + \beta_1x + \mathbb{E}\left[\epsilon|x\right] \\ &= \beta_0 + \beta_1x \end{aligned}$$



## Assumptions

Before interpreting SLR models, one should be aware of its assumptions, namely:
1. The population model is linear in its parameters: $$y = \beta_0 + \beta_1x + \epsilon$$
2. Random bla  
3. There is variation in the $x_i$: $$\sum_{i=1}^n (x_i - \bar{x})^2 > 0$$
4. $\mathbb{E} \left[\epsilon|x\right] = 0$ and therefore $\mathbb{E} \left[\epsilon_i|x_i\right] = 0$
5. Homoskedasticity (Equality of variances) is given: $$Var\left[\epsilon|x\right] = Var\left[\epsilon_i|x_i\right] = \sigma^2$$

Note that this only holds as long as the the assumption of independence is met, i.e. $\frac{\delta \epsilon}{\delta x} = 0$

