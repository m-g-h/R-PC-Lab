---
title: "Probability and Statistics Fundamentals (Session 2)"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
---

# Hello
Blabla this section introduces you to bla

# Random Variables
Random variables are variables that take on numerical values and are determined by experiments. Mathematically speaking, probability experiments (or trials) are procedures which can be infinitely repeated and which have a well-defined set of possible outcomes. These outcomes are affected by chance.

Take for example a simple coin toss experiment. The outcomes are well defined by $\{heads, tails\}$ which we we can assign (recode) to $\{0,1\}$ meaning "no heads" and "heads". The variable for the outcome will be denoted by $X$. Now, unless we perform a cointoss, the value $X$ takes on remains essentially *random*



## A few notational remarks
Consider the vector $X_i = (X_1 , X_2, \dots, X_{100})$ which contains the results from a fair coinflip. Using the `rbinom` function we simulate `n = 100` cointosses and obtain a vector of results $x_i = (x_1, x_2, \dots, x_{100})$ :

```{r}
X <- rbinom(n = 100, size = 1, prob = 0.5)

print(X)

```
 
If we would additionally conduct another experiment where we count two successive coin tosses (`size = 2`), we would obtain a secon random variable, which we would denote by $Y_i$ and $y_i$ respectively.

**Notation:**

- Use **capital letters** such as $O$ or $P$ to denote a **random variable**
- Use **small letters** such as $m$ or $n$ to denote **particual outcomes** of $M$ and $N$
- Use **subscripts** (e.g. $a_i$) to distinct **different instances** of the same random variable

## Discrete random variables
Discrete random variables are variables which can only take a countable number of values. For our cointoss example, $X$ is a random variable with $j = 2$ outcomes, $\{0, 1\}$ (no head, head). Then the probability of each outcome is:

$$p_j = P(X = x_j), \; j = 1, 2 \qquad \text{with } p_1 + p_2 = 1$$  

### Example:drawing a card and predicting its symbol 

- Denote the random variable for the card draw by $C$
- We have $j = 4$ possible outcomes for $C$, $\{hearts, spades, clubs, diamonds\}$
- The probabilities are denoted as: $p_j = P(C = c_j), \; j= 1, 2, \dots , 4$
- Since we know that each symbol is represented equally in the deck, $p_j = 0.25$ for all $j$.

# Probability Density Functions and Cumulative Density Functions

# Features of Probability Distributions

## Central Tendency: The Expected Value

The most commonly used feature of a probability distribution is the *expected value*. It splits the probability under the *pdf* in half.

> The expected value is the value from a probability distribution, for which the probabilities of another value being greater or smaller than it are exactly equal.

For example, we simulate a random variable $R$ from a standard normal distribution. First, we simulate the variable and draw a histogram with the arithmetic mean:
```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337) #seed for reproducibility
data <- data.frame(R = rnorm(n = 1000000))

ggplot(data, aes(x = R)) +
  geom_histogram(aes(y = ..density..), bins = 200) + 
  geom_vline(xintercept = mean(data$R), color="red") +
  theme_minimal()
```

It looks like the area to the left of the mean is the same size at the area on the right side. Let's check that:

```{r}
A <- numeric() # Variable for storing the area sizes

A[1] <- (sum (abs (data$R[data$R < mean(data$R)]))) /sum(abs(data$R)) # Area to the left of the mean in percent
A[2] <- (sum (abs (data$R[data$R > mean(data$R)]))) /sum(abs(data$R)) # Area to the right of the mean in percent

print(A)
```

As we can see (and calculate), the area sizes are very similar, meaning htat half of the probability mass is on each side of the mean. Notice that since we're in a simulation context, the areas are not *exactly* the same.

### Useful Rules for Expected Values

For any constants $a$ and $b$ and a random variable $X$, the following rules apply:

- The expected value of a constant is the constant
$$ \mathbb{E}[c] = c  $$
- The expected value of a linear function of a random variable is the linear function of the expected value of the variable 
$$ \mathbb{E}[aX+b] = a\mathbb{E}[X] + b$$

- The expected value of a sum of random variables is the sum of the expected values of these random variables:
$$ \mathbb{E}\left[ \sum_{i=1}^n a_iX_i \right] = \sum_{i=1}^n \mathbb{E}[a_iX_i] = \sum_{i=1}^n a_i \mathbb{E}[X_i] $$
- For a variable $Y$ that is independent of $X$ it holds that:
$$ \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[X] $$
- The population mean is often denoted by $\mu$ or $\mu_X$ instead of $\mathbb{E}[X]$

## Variablity: Variance and Standard Deviation

The central tendendcy tells us, around which value the outcomes of the random variable cluster, but it is also important to get a measure on how far they *spread around the mean*. The variance and standard deviation are measures for this.

You can see this notion of *distance from the mean* in the definition of the variance:

$$ Var[X] = \mathbb{E} \left[ \left( X - \mathbb{E} [X] \right) ^2  \right] $$

The deviation from the expected value $(X - \mathbb{E}[X])$ is squared in order to prevent perfect cancellation of deviations, since:
$$\mathbb{E} \left[(X - \mathbb{E}[X]) \right] = \mathbb{E}[X] - \mathbb{E}[X] = 0$$

### Useful Rules for Variances

For any constants $a$ and $b$ and a random variable $X$, the following rules apply:

- Alternative representation
$$ Var[X] = \mathbb{E} \left[ X^2 \right] - \mathbb{E}[X]^2 $$
- Variance of a constant is zero:
$$ Var[c] = 0 $$
- Variance of a linear combination:
$$ Var[aX+b] = a^2 Var[X] $$
- The population variance is often denoted by $\sigma^2$ or $\sigma_X^2$ instead of $Var[X]$.

### The Standard Deviation
If we change the unit of measurement of $X$ by $a = 1000$, e.g. from kilometers to meters, the variance increases linearly by $a^2 = 1000 * 1000$ (see rules above). Since this makes it difficult to compare the variability of different variables, we use the standard deviation, which is the positive part of the square root of the variance:

$$ sd[X] = + \sqrt{Var[X]} $$
For any constants $a$ and $b$ and a random variable $X$, it has the following useful properties:
- Standard deviation of a linear combination:
$$ sd[a X + b] = |a| sd[X] $$

- The standard deviation is often denoted by $\sigma$ or $\sigma_X$ instead of $sd[X]$.

## Standardisation of Variables

Using the properties of the expected value and variance, we can transform any variable to a standardised variable with mean 0 and standard deviation 1:
$$ Z := \frac{X-\mu}{\sigma} $$

We can demonstrate this with a short simulation. In the following we simulate a random variable $X$ from a normal distribution, that has mena $3$ and standard deviation $1.5$

```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337) #seed for reproducibility
data_st <- data.frame(R = rnorm(n = 10000, mean = 3, sd = 1.5))

ggplot(data_st, aes(x = R)) +
  geom_histogram(aes(y=..density..),bins = 100) + 
  geom_vline(xintercept = mean(data_st$R), color="red") +
  geom_vline(xintercept = 0, color="green") +
  stat_function(fun = dnorm, color = "green", args = list(mean = 0, sd = 1))+
  stat_function(fun = dnorm, color = "red", args = list(mean = 3, sd = 1.5)) +
  xlim(c(-4,8)) +
  theme_minimal()
```

The green lines indicate a standard normal distribution with $\mu = 0$ and $\sigma = 1$. Our simulated varaible clearly deviates from it. If we now transform $X$ according to the formula above, the result yields:

```{r warning=FALSE, fig.height=4, fig.width=9}
data_st$Z <- (data_st$R - mean(data_st$R)) / sd(data_st$R)

ggplot(data_st, aes(x = Z)) +
  geom_histogram(aes(y=..density..),bins = 100) + 
  geom_vline(xintercept = mean(data_st$R), color="red") +
  geom_vline(xintercept = 0, color="green") +
  stat_function(fun = dnorm, color = "green", args = list(mean = 0, sd = 1))+
  stat_function(fun = dnorm, color = "red", args = list(mean = 3, sd = 1.5)) +
  xlim(c(-4,8)) +
  theme_minimal()
```

## Association: Covariance and Correlation

The covariance captures the joint variation of two random variables. It allows us to get a sense of the relationship of them, e.g. $X$ is always high when $Y$ is. This can be visualised graphically:

```{r fig.height=4, fig.width=9}
library(ggplot2)
set.seed(1337)
data_cov <- data.frame(X = 1:1000 + rnorm(1000, sd = 100),
                       Y = 1:1000 + rnorm(1000, sd = 100))

ggplot(data_cov, aes(x = X, y = Y)) + 
  geom_point() +
  geom_point(aes(x = mean(X), y = mean(Y)), color ="red") + 
  geom_vline(xintercept = mean(data_cov$X) + sd(data_cov$X), color = "red") +
  geom_hline(yintercept = mean(data_cov$Y) + sd(data_cov$Y), color = "red")
```

We can easily see that $X$ and $Y$ have a strong relationship. We can stat that: when the deviation of $X$ (vertical line) from its mean (red dot) is high, the same can be said about the deviation of $Y$ (horizontal line).

Expressing this relation of deviations from the mean mathematically yields us the formula of the covariance:

$$ Cov[X,Y] = \mathbb{E} \left[ (X - \mathbb{E}[X]) (Y - \mathbb{E}{Y} ) \right] $$

### Useful Rules for Covariances

For any constants $a$ and $b$ and random variables $X$, and $Y$ the following rules apply:

- Alternative representations:
$$ \begin{aligned}
Cov[X,Y] &= \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \\
&= \mathbb{E}[(X -\mathbb{E}[X])Y] \\
&= \mathbb{E}[X (Y -\mathbb{E}[Y])] \\
\end{aligned}$$

- Covariance given independence of $X$ and $Y$:
$$ \begin{aligned}
Cov[X,Y] &= \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] \\
&= 0 & \text{since } \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[X] 
\end{aligned}$$

- Variance of linear combinations:
$$ Cov[a_1 X + b_1, a_2Y + b_2] =  a_1 a_2 Cov[X,Y]$$

- The boundary of the covariance of two random variables (its most extreme value) is given by the product of their standard deviations. This is also called the *Cauchy-Schwartz Inequality*:
 $$ |Cov[X,Y]| \leq sd[X] sd[Y] $$
 
 - The population covariance is usually denoted by $\sigma_{XY}$ instead of $Cov[X,Y]$
 
### The Correlation Coefficient

The variance and standard deviation are both dependent on their units of measurement. To capture the *pure* relationsip betwee two random variables, we can get rid of this by using the *Cauchy-Schwartz Inequality* to normalise the variance. This yields us the correlation coefficient:

$$  Corr[X,Y] = \frac{Cov[X,Y]}{sd[X] sd[Y]} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$

- The population correlation coefficient is usually denoted as $\rho$ or $\rho_{XY}$ instead of $Corr[X,Y]$
