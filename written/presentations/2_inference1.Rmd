---
title: "Inference 1 - Estimators and Bias"
output:
  ioslides_presentation:
    widescreen: yes
  slidy_presentation: default
  beamer_presentation:
    theme: metropolis
---

```{r setup, include=FALSE }
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.height=4)
knitr::opts_chunk$set(fig.width=10)
```

## Contents and Goals

Using our knowledge of random variables and their properties, we are now able to conduct some basic inference. This means that we can utilise the *information from a random sample* of i.i.d. variables to learn something about the *population* of these variables.

In this part we're going to:

- Learn what an estimator is 
- Take a a look at the properties of our estimators
- Derive properties of our estimators
  - Bias / Unbiasedness
  - Sampling Distribution
- Introduce Interval Estimation

## Simulated Population

Let's look at an example. Suppose know the population of ZU students income:

```{r echo=TRUE, fig.height=4, fig.width=10}
set.seed(11) # seed for reproducibility

n <- 1200
inc <- rnorm(n, mean = 1000, sd = 200)

print(inc[1:50])
```

## Simulated Population

```{r fig.height=4, fig.width=10}
library(ggplot2)
ggplot() +
  geom_histogram(aes(x = inc, y = ..density..),binwidth = 60,alpha = 0.8) +
  geom_density(aes(x = inc, y = ..density..),col = "#99CC00",size = 2) +
  labs(title = "Population of Income of ZU Students") +
  geom_vline(xintercept = mean(inc),color = "#00CC33",size = 2) +
  geom_vline(xintercept = mean(inc) + 200,color = "#009900",size = 2) +
  geom_vline(xintercept = mean(inc) - 200,color = "#009900",size = 2) +
  theme_minimal()
```

- Population mean: $\mu_{inc} = 1000$
- Population standard deviation: $\sigma_{inc} = 200$

## Simulated Population

Since we have simulated the population, we are in a position where we know:

- The true population mean $\mu_{inc} = 1000€$
- The true population variance $\sigma_{inc}^2$ and standard deviation $\sigma_{inc} = 200$
- The distribution of inc: $inc \sim N(\mu_{inc},\sigma_{inc})$


Suppose now we are interested in the mean of the population (the green line). Given a sample from this distribution, our task now is to estimate the population mean as good as we can.

## Introducing Estimators

Often we don't have knowledge of our population, but have a sample.

From these samples, we can derive knowledge about our population. E.g. its basic moments: the mean and the variance.

Using a sample to derive a moment (or parameter) is called **Estimation**

Generally: 

- If we have a random sample ${X_1, \dots, X_n}$, 
- from a population that depends on some parameter $\theta$
- an estimator $W = f(X)$ is a rule to calculate a certain value
- for *each* possible outcome of the sample

E.g. we can estimate the parameter $\mu$ by calculating the arithemtic mean from a sample.

## Estimating the First Moment / Expected Value

Given a random sample $\{X_1, X_2, \dots, X_n\}$ we can calculate the **sample mean** by:

$$ \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \qquad \text{sample estimator for } \mu_{inc}$$

which is also known as the *arithmetic mean*. Let's simulate a random sample from our ZU students income:

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(24)
sample_1 <- sample(x = inc, size = 50, replace = F)
print(sample_1[1:10])
```

## Estimating the First Moment / Expected Value

```{r message=FALSE, warning=FALSE}
library(tidyverse)
ggplot() +
geom_histogram(aes(x = sample_1,
y = ..density..),binwidth = 50,alpha = 0.8) +
labs(title = "SAMPLE Histogram Income ZU Students",
subtitle = paste0("green = population mean, ",
                  "red = SAMPLE mean:",
                  mean(sample_1) %>% round(2))) +
geom_vline(xintercept = mean(sample_1),color = "red",size = 2) +
  geom_vline(xintercept = mean(inc),color = "#00CC33",size = 2)+
theme_minimal()
```

But how can we draw an inference about the **population mean** $\mu_X$ from this? It could well be that our proposed $\bar{X}$ is a very bad estimator for $\mu_X$.

## Introducing Unbiasedness

- We can analyse many samples of ZU student income
- If our estimator $\bar{X}$ is any good, it should predict the true mean if we do that

Formally, we call this *unbiasedness*:

 - **the estimator derived from a random sample predicts the population parameter on average.**
 
$$ \mathbb{E}[\bar{X}] = \mu_X $$

Or, in other words: our estimator $\bar{X}$ ist distributed with mean $\mu_X$.

The bias of our estimator is then given by:
$$ \begin{aligned}
Bias[\bar{X}] &= \mathbb{E}[\bar{X}] - \mu_X \\
&= 0 & \text{if the estimator is unbiased}
\end{aligned}
$$

## Introducing Unbiasedness

So if $\mathbb{E}[\bar{X}]$ is equal to $\mu_X$, our arithmetic mean would be unbiased. Lets prove this:

<!-- $$\begin{aligned} -->
<!-- \mathbb{E}[\bar{X}] &= \mathbb{E} \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] = \frac{1}{n} \mathbb{E} \left[ \sum_{i=1}^n X_i \right] \\ -->
<!-- &= \frac{1}{n} \sum_{i=1}^n \mathbb{E} [X_i] = \frac{1}{n} \sum_{i=1}^n \mu_X \\ -->
<!-- &= \frac{1}{n} n \mu_X \\ -->
<!-- &= \mu_X -->
<!-- \end{aligned} -->
<!-- $$ -->

$$ \text{ See whiteboard. The proof will be procided in the script, which will be uploaded} $$

## Introducing the Sampling Variance
 
 - Another feature of distributions is their variance
 - The variance **of the mean of** the sample (not *of the sample*) is called *sampling variance*

$$ Var[\bar{X}] = \frac{\sigma_X^2}{n} $$

The proof of this is very similar to the proof for the sample mean.

The sampling standard deviation is then given by:
$$ sd[\bar{X}] = \frac{\sigma_X}{\sqrt{n}} $$

## The Distribution of our Estimator

- We know now now the expected value and the sampling variance of our estimator $\bar{X}$:

$$ \mathbb{E}[\bar{inc}] = \mu_{inc} = 1000 $$
$$sd[\bar{inc}]= \frac{\sigma_{inc}}{\sqrt{n}} = \frac{200}{\sqrt{50}} = 28.28$$

- Since $\bar{X}$ is a linear function of normally distributed variables, it is also distributed normally.

## The Distribution of our Estimator

```{r fig.height=3.5}
ggplot(data =  data.frame(X_bar = 900:1100), aes(x=X_bar)) +
  stat_function(fun = dnorm, args = list(mean = 1000, sd = 200/sqrt(50)), size = 2) +
  geom_vline(xintercept = mean(sample_1), color = "red", size = 1.5) +
  geom_vline(xintercept = 1000, color = "#00CC33", size = 1.5) +
  geom_vline(xintercept = 910, color = "blue", size = 1.5) +
  labs(title = "PDF of our sample mean") +
  theme_minimal()
```

- Observing a mean ZU student income of  $\bar{X} = 998.4€$ from a random sample seems to be very likely
- A mean income of $910€$ (blue line), is observed less often. Maybe the sample was from another (not private) university? ;)


## Short Recap

**What have we learned so far?**

- We can use a function of observations from a sample to estimate parameters from the population
    - E.g. we can use a function like $\sum_{i=1}^n X_i$ to estimate $\mu_X$
- These estimators have a distribution. If the expected value of the estimator is equal to the estimated parameter, we call it *unbiased*
    - E.g. if $\mathbb{E}[\bar{X}] = \mu_X + 50€$ we would overestimate ZU student income
- If we know the population standard deviation $\sigma$, we can calculate the sampling variation of our estimator and tell how likely the sample is from the population
    
## Interval estimation

- Given the sampling variance and expected value of the estimator, we can also standardise it:

$$ Z= \frac{X - \mu_X}{\sigma_X} \qquad \text{general z-standardisation formula} $$
$$ Z_{\bar{inc}} = \frac{\bar{inc} - \mu_{inc}}{\sigma_{inc} / \sqrt{n}} = \frac{998.4 -1000}{28.28} = - 0.056$$

- This helps us to calculate probabilities, since now our estimator is distributed *standard normally*

## Interval estimation
 $95 \%$ of the values of a standard normal variable lie in the interval $[-1.96, 1.96]$ (see [z-table](http://www.z-table.com/)) or use `qnorm(0.95)` in R

```{r}
ggplot(data =  data.frame(X_bar = -4:4), aes(x=X_bar)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), size = 2) +
  stat_function(fun = dnorm, xlim = c(-1.96,1.96), geom = "area", fill = "darkgrey", alpha=0.5) +
  stat_function(fun = dnorm, xlim = c(-4,-1.96), geom = "area", fill = "black", alpha=0.5) +
   stat_function(fun = dnorm, xlim = c(1.96,4), geom = "area", fill = "black", alpha=0.5) +
  geom_vline(xintercept = (mean(sample_1) - 1000) / (200/sqrt(50)), color = "red", size = 1.5) +
geom_vline(xintercept = (910 - 1000) / (200/sqrt(50)), color = "blue", size = 1.5) +
  geom_vline(xintercept = 0, color = "green", size = 1.5) +
  labs(title = "Standardised PDF of our sample mean") +
  theme_minimal()
```


## Confidence Interval

We can also construct an interval around our sampled mean income $\bar{inc}$ that contains $\mu_{inc}$ with $1- \alpha \%$ probability:

$$ \left[ \bar{x} - \frac{z_{1-\alpha/2}\sigma_X }{\sqrt{n}}, \bar{x} + \frac{z_{1-\alpha/2} \sigma_X }{\sqrt{n}}   \right]  \qquad \text{general confidence interval formula}$$

E.g. a $95 \%$ confidence interval for our estimated ZU student income:

$$ \begin{aligned} 
\left[ 998.4 - \frac{1.96 *200 }{\sqrt{50}}, 998.4 + \frac{1.96 *200 }{\sqrt{50}}   \right] 
&= [942.96,1053,84] 
\end{aligned}$$


## Confidence Interval

```{r}
ggplot(data =  data.frame(X_bar = 900:1100), aes(x=X_bar)) +
  stat_function(fun = dnorm, args = list(mean = 1000, sd = 200/sqrt(50)), size = 2) +
  geom_vline(xintercept = mean(sample_1), color = "red", size = 1.5) +
  geom_vline(xintercept = 1000, color = "green", size = 1.5) +
  geom_vline(xintercept = mean(sample_1) - (1.96 * 200 /sqrt(50)), color = "darkred", size = 1.5) +
  geom_vline(xintercept = mean(sample_1) + (1.96 * 200 /sqrt(50)), color = "darkred", size = 1.5) +
  labs(title = "PDF of our sample mean") +
  theme_minimal()
```


So we can in $95 \%$ of cases our true mean ZU student income lies in the interval denoted by the red lines, in this case $[987,1009]$. 

## Confidence Interval

But we could also observe in, $5\%$ of cases, interval estimates that don't contain the true mean ZU student income:

```{r}
ggplot(data =  data.frame(X_bar = 900:1100), aes(x=X_bar)) +
  stat_function(fun = dnorm, args = list(mean = 1000, sd = 200/sqrt(50)), size = 2) +
  geom_vline(xintercept = 910, color = "blue", size = 1.5) +
  geom_vline(xintercept = 1000, color = "green", size = 1.5) +
  geom_vline(xintercept = 910 - (1.96 * 200 /sqrt(50)), color = "darkblue", size = 1.5) +
  geom_vline(xintercept = 910 + (1.96 * 200 /sqrt(50)), color = "darkblue", size = 1.5) +
  labs(title = "PDF of our sample mean") +
  theme_minimal()
```

## Confidence intervals: The Whole Picture

If we take 500 samples and construct their confidence intervals for $95\%$ we get: 

```{r message=FALSE, warning=FALSE}
library(tidyverse)
nn <- 500
set.seed(11)
samples <- map(.x = 1:1000,
               .f = ~ sample(inc,size = 50,replace = FALSE))
means <- map_dbl(samples[1:nn],~mean(.x))
se <- map_dbl(samples[1:nn],~ sd(.x)/sqrt(length(.x)))
lower = means-1.96*se
upper = means+1.96*se
df <- data.frame(means,lower,upper)[order(means),] %>%
  mutate(order = 1:nrow(.),
         coll = ifelse(upper < 1000 | lower > 1000,"missed","ok"))
ggplot(df) +
  geom_point(aes(x = order,
                 y = means,
                 col = coll)) +
  geom_segment(aes(x = order-0.02,xend = order + 0.02,
                   y = lower,yend = upper,col = coll)) +
  labs(title = "500 Sample 95% confidence interval") +
  scale_color_discrete(name = "everything ok?")+
  geom_hline(aes(yintercept =1000)) +
  theme_minimal()
```

(Props to Marcel Schliebs for this visualisation)

## Estimating the Second Moment / Variance

An unbiased estimator of the first moment -- the *sample variance* -- is given by:

$$ S^2 = \frac{1}{n-1} \sum_{i=1}^2 (X_i - \bar{X})^2 $$

- It is used when the population variance and standard devation are not known

We divide by $\frac{1}{n-1}$ because we use $\bar{X}$ and not $\mu_X$. The details can be looked up  [here](https://en.wikipedia.org/wiki/Variance#Sample_variance).

## Estimating the Second Moment / Variance

The sample standard deviation is given by:
$$ S = \sqrt{S^2} $$

This estimator is not unbiased. However it is a *consistent* estimator for $\sigma$ and we can still use it. For the sake of brevity we will skip the discussion of consistency here.

## Summary of Estimators and Bias

- Standardising our estimator helps us to calculate probabilities
    - In our case, $\bar{X}$ became distributed standard normal after standardisation, because $X$ is normal distributed
- We can easily look up z-values in a z-table or with `qnorm`
- We can construct confidence intervals around our estimates that are derived from our sampling procedure
    - The population parameter is contained within this intervail with a certain probability